{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "from raiSimulationEnvATKDEF import RobotSimEnv\n",
    "import robotic as ry\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, log_file: str, verbose: int = 0):\n",
    "        super(RewardLoggerCallback, self).__init__(verbose)\n",
    "        self.log_file = log_file\n",
    "        self.episode_rewards = []\n",
    "        self.current_episode_reward = 0\n",
    "\n",
    "        # Create the log file if it doesn't exist\n",
    "        if not os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'w') as f:\n",
    "                f.write(\"Episode,Total Reward\\n\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if the episode has ended by using `done`\n",
    "        dones = self.locals[\"dones\"]\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "\n",
    "        # Accumulate rewards for the current episode\n",
    "        self.current_episode_reward += rewards[0]\n",
    "\n",
    "        # If the episode is done, log the reward\n",
    "        if dones[0]:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(f\"{len(self.episode_rewards)},{self.current_episode_reward}\\n\")\n",
    "            # Reset the reward counter for the next episode\n",
    "            self.current_episode_reward = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Optionally summarize results at the end of training\n",
    "        print(\"Training finished. Total episodes:\", len(self.episode_rewards))\n",
    "        print(\"Episode rewards:\", self.episode_rewards)\n",
    "\n",
    "class CustomCheckpointCallback(BaseCallback):\n",
    "    def __init__(self, save_freq, save_path, verbose=0):\n",
    "        super(CustomCheckpointCallback, self).__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Save the model every `save_freq` steps\n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            model_path = f\"{self.save_path}/model_checkpoint_{self.n_calls}_steps.zip\"\n",
    "            self.model.save(model_path)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Model saved at step {self.n_calls} to {model_path}\")\n",
    "        return True\n",
    "\n",
    "class StaticOpponentWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper for a 1v1 environment where one agent is controlled by a fixed, pre-trained policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, static_policy_attacker, static_policy_defender,attacker=True,test=False):\n",
    "        super(StaticOpponentWrapper, self).__init__(env)\n",
    "        self.static_policy_attacker = static_policy_attacker\n",
    "        self.static_policy_defender = static_policy_defender\n",
    "        self.attacker = attacker\n",
    "        self.test = test\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get the static opponent's action\n",
    "        obs = self.env.state  # Modify this based on how your env works\n",
    "        attacker_action, _ = self.static_policy_attacker.predict(obs, deterministic=True)\n",
    "        defender_action, _ = self.static_policy_defender.predict(obs, deterministic=True)\n",
    "        # Combine both actions into a joint action\n",
    "        if self.attacker:\n",
    "            joint_action = (attacker_action, action)\n",
    "        else:\n",
    "            joint_action = (action,defender_action)\n",
    "        \n",
    "        if self.test:\n",
    "            joint_action = (attacker_action, defender_action)\n",
    "\n",
    "        # Step the environment with both actions\n",
    "        obs, reward, done, truncated, info = self.env.step(joint_action)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = RobotSimEnv(render_mode='human')\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[256, 256])]  # 'pi' is the actor network, 'vf' is the critic network\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=0)\n",
    "\n",
    "reward_logger = RewardLoggerCallback(log_file=\"rewards_log_baranpath.csv\")\n",
    "model.learn(total_timesteps=100000,progress_bar=True,callback=reward_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/said/robotics/lib/python3.9/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/said/robotics/lib/python3.9/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "Success ('sword_1', 'r_panda_coll2', -0.0014683094257100515)\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "Success ('sword_1', 'r_panda_coll6', -0.024026262069100385)\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "Success ('sword_1', 'r_panda_coll4', -0.006224055909788528)\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "Success ('sword_1', 'r_panda_coll2', -0.0024894655552762796)\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import time\n",
    "\n",
    "env = RobotSimEnv(render_mode='human',staticAttacker=True, staticDefender=True)\n",
    "\n",
    "attackModel = PPO.load(\"best_sword_model_attack\")\n",
    "defenceModel = PPO.load(\"shield_model_best\")\n",
    "\n",
    "wrapped_env = StaticOpponentWrapper(env, attackModel,defenceModel,attacker=True,test=True)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[256, 256])]  # 'pi' is the actor network, 'vf' is the critic network\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", wrapped_env, policy_kwargs=policy_kwargs, verbose=0, device='cpu')   \n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "#env.render()\n",
    "time.sleep(1)\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    env.render()\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT l_panda_joint7-sword_0 of type rigid with rel [0, 0, 0]\n",
      "-- kin_physx.cpp:addJoint:298(0) ADDING JOINT r_panda_joint7-shi of type rigid with rel [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "import time\n",
    "\n",
    "env = RobotSimEnv(render_mode='human')\n",
    "model = SAC.load(\"sac_baranpath\",env)\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "#env.render()\n",
    "time.sleep(1)\n",
    "for i in range(200):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    env.render()\n",
    "    # VecEnv resets automatically\n",
    "    # if done:\n",
    "    #   obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
